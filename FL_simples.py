# -*- coding: utf-8 -*-
"""FL_simple.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tBmlCyMK0IKHMMVT9PUXHHOH6XlZdhwQ
"""

#!pip install tensorflow

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import shuffle

# Carregar o conjunto de dados NSL-KDD

!wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz
!gzip -d kddcup.data_10_percent.gz

df = pd.read_csv('/content/KDDTrain+.txt')

# Definir as colunas
columns = np.array([
    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',
    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',
    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',
    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',
    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',
    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',
    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',
    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',
    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'
])

# Carregar os dados
#data = np.genfromtxt('KDDTrain+.txt', delimiter=',')
data = np.genfromtxt('/content/KDDTrain+.txt', delimiter=',')
X = data[:, :-1]
y_str = data[:, -1].astype(object)

data

# Pré-processamento dos dados
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Codificação one-hot para variáveis categóricas
X_encoded = np.zeros((X.shape[0], X.shape[1] + 3))
for i in range(X.shape[1]):
    if i not in [1, 2, 3]:  # As colunas 1, 2 e 3 são variáveis categóricas
        X_encoded[:, i] = X_scaled[:, i]
    else:
        unique_values = np.unique(X[:, i])
        for j, value in enumerate(unique_values):
            X_encoded[:, j + X.shape[1]] = (X[:, i] == value).astype(int)

# Codificar os rótulos
labels = np.unique(y_str)
y_encoded = np.array([np.where(labels == label)[0][0] for label in y_str])

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Definir a arquitetura do modelo
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(len(labels), activation='softmax')
])

# Compilar o modelo
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

print(X_test.shape)
print(y_test.shape)

# Treinar o modelo localmente
model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test))

# Função para dividir os dados em shards
def create_shards(X, y, num_clients):
    X_shards, y_shards = [], []
    X, y = shuffle(X, y)
    shard_size = len(X) // num_clients
    for i in range(num_clients):
        start = i * shard_size
        end = (i + 1) * shard_size if i < num_clients - 1 else len(X)
        X_shards.append(X[start:end])
        y_shards.append(y[start:end])
    return X_shards, y_shards

# Configurações do Federated Learning
num_clients = 5
num_rounds = 10
batch_size = 32

# Dividir os dados em shards para os clientes
X_shards, y_shards = create_shards(X_train, y_train, num_clients)

# Iniciar o treinamento federado
for round_num in range(num_rounds):
    print(f"Round {round_num + 1}/{num_rounds}")
    for client in range(num_clients):
        print(f"\tClient {client + 1}/{num_clients}")
        model_weights = model.get_weights()
        client_X, client_y = X_shards[client], y_shards[client]
        client_dataset = tf.data.Dataset.from_tensor_slices((client_X, client_y)).shuffle(len(client_X)).batch(batch_size)
        client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
        for X_batch, y_batch in client_dataset:
            with tf.GradientTape() as tape:
                y_pred = model(X_batch)
                loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred)
            gradients = tape.gradient(loss, model.trainable_variables)
            client_optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    print("\tEvaluating global model")
    print(f"\tRound {round_num + 1} Accuracy:", model.evaluate(X_test, y_test, verbose=0)[1])

print("Federated Learning Finished")